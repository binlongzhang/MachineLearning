{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c820dfa-268d-4454-a207-d54e38ae6b02",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯\n",
    "- 优点\n",
    "    - 在数据较少时仍旧有效，可以处理多类别问题\n",
    "- 缺点\n",
    "    - 对于输入数据的准备方式敏感\n",
    "- 适用数值类型：标称性数据\n",
    "\n",
    "贝叶斯决策论的核心思想就是选择具有高概率的决策\n",
    "\n",
    "贝叶斯概率引入先验知识和逻辑推理来处理不确定命题，另一种概率解释称为frequency probability，它只从数据本身获取结论而不考虑推理及先验知识\n",
    "\n",
    "贝叶斯公式帮助我们交换条件概率中的条件与结果\n",
    "$$\n",
    "p(c|x) = \\frac{p(x|c)p(c)}{p(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a4d7b-ab01-456f-926f-71e318862bef",
   "metadata": {},
   "source": [
    "# 使用NB进行文本分类\n",
    "> 朴素贝叶斯假设每个特征同等重要且相互独立，这也正是其名字的由来，尽管这些假设并不合理，但朴素贝叶斯确实起到了不错的效果\n",
    "\n",
    "- 从文本中获取特征需要拆分文本\n",
    "- 此处特征是来自文本的词条（token），其可以是任意字符组合，可以理解为单词，也可以使用非单词词条如url，IP地址等等\n",
    "- 还需要将每个文本片段表示为一个词向量，1表示出现，0表示未出现\n",
    "\n",
    "以在线社区留言板为例，为了不影响社区发展，需要屏蔽侮辱性言论，因此需要构建一个快速过滤器，对此需要设置连个类别：侮辱和非侮辱，使用1和0表示\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc7c79-6e66-42f8-b02d-a1ce3dd504e6",
   "metadata": {},
   "source": [
    "## 准备数据：从文本中构建词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d89a39-1eda-4809-a3c5-3d0970ee9d18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flea', 'food', 'so', 'garbage', 'please', 'mr', 'buying', 'problems', 'licks', 'take', 'steak', 'my', 'not', 'ate', 'maybe', 'has', 'dog', 'stupid', 'cute', 'I', 'is', 'dalmation', 'love', 'how', 'posting', 'him', 'help', 'worthless', 'stop', 'quit', 'park', 'to']\n"
     ]
    }
   ],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    # 1表示侮辱性词汇，0表示没有\n",
    "    classVec = [0,1,0,1,0,1]\n",
    "    return postingList,classVec\n",
    "\n",
    "# 创建词汇列表\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  \n",
    "    for document in dataSet:\n",
    "        # 合并两个集合\n",
    "        vocabSet = vocabSet | set(document) \n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print('the word: %s is not in Vocabulary!'%word)\n",
    "    return returnVec\n",
    "\n",
    "listOPosts,listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "print(myVocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a3032c6-e657-4163-b351-65d1d3c623d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(setOfWords2Vec(myVocabList,listOPosts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ca33b-1768-4fc4-a68f-48f5b690db9d",
   "metadata": {},
   "source": [
    "## 训练算法：从词向量计算概率\n",
    "W表示词向量，c表示类别那么贝叶斯公式可以表示为$ p(c_i|w) = \\frac{p(w|c_i)p(c_i)}{p(w)}$, 为此我们需要\n",
    "1. 首先获得$p(c_i)$,通过计算每个类别在总文档中所占比例\n",
    "2. 由于朴素贝叶斯假设$p(w|c_i) = p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)\\cdots p(w_n|c_i)$,由此可得$p(w|c_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "804a13c5-d3ae-40ef-a9c6-62779aae0e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "# 朴素贝叶斯分类器训练函数\n",
    "def trainNB0(trainData,TrainClass):\n",
    "    numTrainDocs = len(trainData)\n",
    "    numWords = len(trainData[0])\n",
    "    proAbusive = sum(TrainClass)/float(numTrainDocs)\n",
    "    \n",
    "    pro0Num = zeros(numWords)\n",
    "    pro1Num = zeros(numWords)\n",
    "    pro0Denom = 0.0\n",
    "    pro1Denom = 0.0\n",
    "    \n",
    "    for i in range(numTrainDocs):\n",
    "        if TrainClass[i]==1:\n",
    "            pro1Num += trainData[i]\n",
    "            pro1Denom += sum(trainData[i])\n",
    "        else:\n",
    "            pro0Num += trainData[i]\n",
    "            pro0Denom += sum(trainData[i])\n",
    "    pro1Vec = pro1Num/pro1Denom\n",
    "    pro0Vec = pro0Num/pro0Denom\n",
    "    \n",
    "    return pro0Vec,pro1Vec,proAbusive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55de3e30-a56f-474f-9e8f-4f1836710e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOPosts,listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList,postinDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d0ff608-120b-4ddb-8176-62986c1a0c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(trainMat)):\n",
    "    print(trainMat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05165f61-33ac-4e30-b747-8bf0bd9f9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p0V,p1V,pAb = trainNB0(trainMat,listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "086616de-b290-44d9-9139-248c2cb4bded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[0.04166667 0.         0.04166667 0.         0.04166667 0.04166667\n",
      " 0.         0.04166667 0.04166667 0.         0.04166667 0.125\n",
      " 0.         0.04166667 0.         0.04166667 0.04166667 0.\n",
      " 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667\n",
      " 0.         0.08333333 0.04166667 0.         0.04166667 0.\n",
      " 0.         0.04166667]\n",
      "[0.         0.05263158 0.         0.05263158 0.         0.\n",
      " 0.05263158 0.         0.         0.05263158 0.         0.\n",
      " 0.05263158 0.         0.05263158 0.         0.10526316 0.15789474\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.05263158 0.05263158 0.         0.10526316 0.05263158 0.05263158\n",
      " 0.05263158 0.05263158]\n"
     ]
    }
   ],
   "source": [
    "print(pAb)\n",
    "print(p0V)\n",
    "print(p1V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc3859c-7a0c-4f81-a97f-a5c83d836f65",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 测试算法： 根据实际情况改进分类器\n",
    "利用贝叶斯分类器对文档分类时，需要计算多个概率乘积获得某个文档分配的概率，即\n",
    "$$p(w_0|c_0)p(w_1|c_0)\\cdots p(w_n|c_0)$$\n",
    "---\n",
    "当其中一个概率为0时，最后乘积也是0，为了减低该影响，可以进行Laplace校准\n",
    "$$\n",
    "p_\\lambda(X^{(j)}=a_{jI}|Y=c_k) = \\frac{\\sum^N_{i=1}I(x_i^{(j)} = a_{jI},y_i=c_k)+\\lambda}{\\sum^N_{i=1}I(y_i=c_k)+S_j\\lambda}\n",
    "$$\n",
    "其中$a_jl$表示j个特征的第I个属性，$S_J$表示第j个特征的属性个数，k代表种类个数\n",
    "\n",
    "---\n",
    "此外由于每个$p(w_i|1)$都小于1，多个很小数字相乘会出现下溢，一种有效的解决方法是对小数取自然对数，$ln(a*b) = ln(a)+ln(b)$于是通过求对数可以避免下溢\n",
    "\n",
    "$f(x)和ln(f(x))$**同增减**，且在相同点上取极值，虽然取值不同但不影响结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d86569-7a60-46ec-ab02-fa5736eadf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainData,TrainClass):\n",
    "    numTrainDocs = len(trainData)\n",
    "    numWords = len(trainData[0])\n",
    "    proAbusive = sum(TrainClass)/float(numTrainDocs)\n",
    "    \n",
    "    pro0Num = ones(numWords)\n",
    "    pro1Num = ones(numWords)\n",
    "    pro0Denom = 2.0\n",
    "    pro1Denom = 2.0\n",
    "    \n",
    "    for i in range(numTrainDocs):\n",
    "        if TrainClass[i]==1:\n",
    "            pro1Num += trainData[i]\n",
    "            pro1Denom += sum(trainData[i])\n",
    "        else:\n",
    "            pro0Num += trainData[i]\n",
    "            pro0Denom += sum(trainData[i])\n",
    "    pro1Vec = log(pro1Num/pro1Denom)\n",
    "    pro0Vec = log(pro0Num/pro0Denom)\n",
    "    \n",
    "    return pro0Vec,pro1Vec,proAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f540ff81-a898-4549-9452-d6bc1f114a99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation']  classified as :  0\n",
      "['stupid', 'garbage']  classified as :  1\n"
     ]
    }
   ],
   "source": [
    "def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):\n",
    "    # 这里由于是对数形式因此相加等于相乘\n",
    "    p1 = sum(vec2Classify*p1Vec) + log(pClass1)\n",
    "    p0 = sum(vec2Classify*p0Vec) + log(1-pClass1)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    testEntry = ['love','my','dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList,testEntry))\n",
    "    print(testEntry,' classified as : ' ,classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "    testEntry = ['stupid','garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList,testEntry))\n",
    "    print(testEntry,' classified as : ' ,classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "    \n",
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6dde06-a6f4-4a53-8b33-6851c510cd7a",
   "metadata": {},
   "source": [
    "## 准备数据： 文档词袋模型\n",
    "- 目前为止，我们将每个词是否出现作为特征，其被称为set-of-words model\n",
    "- 如果一个词在文档出现不止一次，该方法不能体现该信息，这种方法被称为 bags-of-words model\n",
    "- 在词袋模型中每个单词可以出现多次，词集中每个词只能出现一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13cf03fa-2072-4444-a3cf-c2d1f8660ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 朴素贝叶斯词袋模型\n",
    "def bagOfWords2VecMN(vocabList,inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf0cd1f0-185d-441e-b81d-72a23ff600a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "testEntry = ['love','my','dalmation','my']\n",
    "print(bagOfWords2VecMN(myVocabList,testEntry))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2168dbd2-7651-427c-83f8-1a70c1098ccc",
   "metadata": {},
   "source": [
    "# 使用NB过滤垃圾邮件\n",
    "1. 收集数据：提供文本文件\n",
    "2. 准备数据：将文本解析成词条向量\n",
    "3. 分析数据：检查词条确保解析的正确性\n",
    "4. 训练算法：使用之前的建立的trainNB0()函数\n",
    "5. 测试算法：使用classifyNB()测试，并鬼剑测试函数计算文档的错误率\n",
    "6. 使用算法：构建完整程序对文档进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61edcb1e-d37a-4088-9ded-73d9d0c1e554",
   "metadata": {},
   "source": [
    "## 准备数据：切分文本，构建词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5e99549-b8c6-44a7-a011-8bed8ecd1ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "mySent = 'This book is the best book on Python or M.L. I have ever laid eyes upon.'\n",
    "# 使用正则表达式清楚标点符号，不过有些情况下标点对于文本语义的表达也很重要\n",
    "regEx = re.compile('\\W+')\n",
    "listofTokens = regEx.split(mySent)\n",
    "print(listofTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddcc2e09-2172-4160-a25a-880a13982799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'm', 'l', 'i', 'have', 'ever', 'laid', 'eyes', 'upon']\n"
     ]
    }
   ],
   "source": [
    "# 实际上句子查找，首字母大写很重要，但是对于此处的文本仅仅看做词袋，因此我们需要统一文本形式\n",
    "print([tok.lower() for tok in listofTokens if len(tok)>0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f35c408-f04c-49e5-bd3d-4a58d19ac7de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Since', 'you', 'are', 'an', 'owner', 'of', 'at', 'least', 'one', 'Google', 'Groups', 'group', 'that', 'uses', 'the', 'customized', 'welcome', 'message', 'pages', 'or', 'files', 'we', 'are', 'writing', 'to', 'inform', 'you', 'that', 'we', 'will', 'no', 'longer', 'be', 'supporting', 'these', 'features', 'starting', 'February', '2011', 'We', 'made', 'this', 'decision', 'so', 'that', 'we', 'can', 'focus', 'on', 'improving', 'the', 'core', 'functionalities', 'of', 'Google', 'Groups', 'mailing', 'lists', 'and', 'forum', 'discussions', 'Instead', 'of', 'these', 'features', 'we', 'encourage', 'you', 'to', 'use', 'products', 'that', 'are', 'designed', 'specifically', 'for', 'file', 'storage', 'and', 'page', 'creation', 'such', 'as', 'Google', 'Docs', 'and', 'Google', 'Sites', 'For', 'example', 'you', 'can', 'easily', 'create', 'your', 'pages', 'on', 'Google', 'Sites', 'and', 'share', 'the', 'site', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '174623', 'with', 'the', 'members', 'of', 'your', 'group', 'You', 'can', 'also', 'store', 'your', 'files', 'on', 'the', 'site', 'by', 'attaching', 'files', 'to', 'pages', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '90563', 'on', 'the', 'site', 'If', 'you抮e', 'just', 'looking', 'for', 'a', 'place', 'to', 'upload', 'your', 'files', 'so', 'that', 'your', 'group', 'members', 'can', 'download', 'them', 'we', 'suggest', 'you', 'try', 'Google', 'Docs', 'You', 'can', 'upload', 'files', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '50092', 'and', 'share', 'access', 'with', 'either', 'a', 'group', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '66343', 'or', 'an', 'individual', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '86152', 'assigning', 'either', 'edit', 'or', 'download', 'only', 'access', 'to', 'the', 'files', 'you', 'have', 'received', 'this', 'mandatory', 'email', 'service', 'announcement', 'to', 'update', 'you', 'about', 'important', 'changes', 'to', 'Google', 'Groups', '']\n"
     ]
    }
   ],
   "source": [
    "emailText = open('../data/NaiveBayes/email/ham/6.txt').read()\n",
    "listOfTokens = regEx.split(emailText)\n",
    "print(listOfTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8d9b33-fb34-48ef-85c3-26bfb479c192",
   "metadata": {},
   "source": [
    "上述读取的是某公司告知不在进行某些支持的一封邮件，由于其中包含URL因此会出现一部分无意义单词（en,py等）,想去掉这些无意义单词，一种简单的方式是过滤长度小于3的字符串，本例通过使用一个通用的文本解析规则实现这一点\n",
    "\n",
    "实际的解析程序中，要用更加高级的过滤器对HTML，URL对象进行处理，当前一个URL最后被解析成词汇表中的单词，文本解析是一个相当复杂的过程，在此没有进行深入的尝试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdcace5-e974-44aa-9de7-4351f7552cbf",
   "metadata": {},
   "source": [
    "## 测试算法：NB进行交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eb2308c-3838-405e-9ada-fbda1e741fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本解析\n",
    "def textParse(bigString):\n",
    "    listOfTokens = re.split(r'\\W+',bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "\n",
    "def spamTest():\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    # 获得解析文本\n",
    "    for i in range(1,26):\n",
    "        wordList = textParse(open('../data/NaiveBayes/email/spam/%d.txt'%i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.append(wordList)\n",
    "        classList.append(1)    \n",
    "        wordList = textParse(open('../data/NaiveBayes/email/ham/%d.txt'%i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.append(wordList)\n",
    "        classList.append(0)\n",
    "\n",
    "    # 构造词汇表    \n",
    "    vocabList = createVocabList(docList)\n",
    "\n",
    "    # 随机构造训练，测试集\n",
    "    trainingSetIndex = [x for x in range(50)] \n",
    "    testSet = []\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0,len(trainingSetIndex)))\n",
    "        testSet.append(trainingSetIndex[randIndex])\n",
    "        del(trainingSetIndex[randIndex])\n",
    "\n",
    "    # 构造训练集\n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSetIndex:\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList,docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    # 构造模型\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "\n",
    "    # 测试\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = bagOfWords2VecMN(vocabList,docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print('the error rate is : ',str(errorCount/len(testSet)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c2c60de-6f3c-44e5-a691-4783a21a458e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is :  0.2\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973b4110-08a9-4723-9e8a-be4fbbf1234a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 使用NB从个人广告中获取区域倾向\n",
    "\n",
    "这个例子中，我们分别从美国两个城市选一批人，通过分析他们发布的征婚广告的信息来比较两个城市的人们在广告用词上是否不同。如果不同，他们各自常用的词是哪些？从人们用词中，考虑是否能对不同城市的人所关心的内容有所了解呢？\n",
    "\n",
    "---\n",
    "\n",
    "1. 收集数据：从RSS源收集内容，这里需要对RSS源构建一个接口\n",
    "2. 准备数据：将文本文件解析成词条向量\n",
    "3. 分析数据：检查词条确保解析正确\n",
    "4. 训练算法：使用我们之前建立的trainNB0()函数\n",
    "5. 测试算法：观察错误率，确保分类器可用。可以修改切分程序，降低错误率，提高分类结果\n",
    "6. 使用算法：构建一个完整程序，封装代码，给定两个RSS源，显示常用公共词\n",
    "---\n",
    "使用来自不同城市的广告训练一个分类器，目的并非使用该分类器进行分类，而是通过观察单词和条件概率值来发现特定城市的相关的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84835f8-1bf5-463f-b55c-431c132a0343",
   "metadata": {},
   "source": [
    "## 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e42d29a0-d9e6-4aac-89be-dd87aaffa607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用feedparser解析Rss数据\n",
    "import feedparser\n",
    "nasa = feedparser.parse('http://www.nasa.gov/rss/dyn/image_of_the_day.rss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a49e6a6f-13ea-40f9-b579-cc7c1ead6c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nasa['entries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8eee1b24-f8a4-45e0-a5f0-edf83a81dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "history = feedparser.parse('https://www.sup.org/rss/?feed=history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23e4a57d-0b05-4b21-a3bb-a46fd96de099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(history['entries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "205a6e9a-33e7-4105-9689-cecf532648e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def calcMostFreq(vocabList,fullText):\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token] = fullText.count(token)\n",
    "    sortedFreq = sorted(freqDict.items(),key = operator.itemgetter(1),reverse=True)\n",
    "    return sortedFreq[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "019009f2-aa83-432e-ba6f-e09f7bbc5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "def localWords(feed0,feed1):\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "\n",
    "    minLen = min(len(feed1['entries']),len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "    vocabList = createVocabList(docList)\n",
    "\n",
    "    # 去除出现次数最多的词\n",
    "    top30Words = calcMostFreq(vocabList,fullText)\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList:\n",
    "            vocabList.remove(pairW[0])\n",
    "\n",
    "    # 随机构造训练测试集\n",
    "    trainingSetIndex = [x for x in range(2*minLen)] \n",
    "    testSet = []\n",
    "    for i in range(20):\n",
    "        randIndex = int(random.uniform(0,len(trainingSetIndex)))\n",
    "        testSet.append(trainingSetIndex[randIndex])\n",
    "        del(trainingSetIndex[randIndex])\n",
    "\n",
    "    # 构造训练集\n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSetIndex:\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList,docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "\n",
    "    # 构造模型\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "\n",
    "    # 测试\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = bagOfWords2VecMN(vocabList,docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print('the error rate is : ',str(errorCount/len(testSet)))\n",
    "    \n",
    "    return vocabList,p0V,p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0797f8a3-6cbf-4da8-9b5e-e0a2bf41a681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is :  0.5\n"
     ]
    }
   ],
   "source": [
    "vocabList,p0V,p1V = localWords(nasa,history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7985510a-c05b-4044-9f7d-85f1497bd6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1457"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348574c8-5dd4-4b22-9cd6-18e1af763170",
   "metadata": {},
   "source": [
    "**此处错误率远高于垃圾邮件中的错误率，但由于这里关注的是单词概率而不是实际分类，因此这个问题不重要**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ddaa5-652b-486f-acf7-6de3124ea0ee",
   "metadata": {},
   "source": [
    "## 分析数据：显示相关用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7d76b91-771d-4f72-96aa-9778ce2ab24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is :  0.45\n",
      "**********NASA**********\n",
      "2021: -2.890371757896165\n",
      "dec: -2.890371757896165\n",
      "crew: -2.890371757896165\n",
      "aboard: -3.1780538303479458\n",
      "launch: -3.1780538303479458\n",
      "nasa: -3.1780538303479458\n",
      "rocket: -3.1780538303479458\n",
      "station: -3.1780538303479458\n",
      "spacex: -3.1780538303479458\n",
      "image: -3.1780538303479458\n",
      "**********History**********\n",
      "iran: -4.785107951426769\n",
      "benatar: -4.785107951426769\n",
      "sex: -4.785107951426769\n",
      "police: -4.939258631254028\n",
      "work: -4.939258631254028\n",
      "arani: -4.939258631254028\n",
      "years: -4.939258631254028\n"
     ]
    }
   ],
   "source": [
    "vocabList,p0V,p1V = localWords(nasa,history)\n",
    "topNASA = []\n",
    "topHis = []\n",
    "for i in range(len(p0V)):\n",
    "    if p0V[i] > -3.5:\n",
    "        topNASA.append((vocabList[i],p0V[i]))\n",
    "    if p1V[i] > -5:\n",
    "        topHis.append((vocabList[i],p1V[i]))\n",
    "        \n",
    "print('*'*10 + 'NASA' + '*'*10)\n",
    "sortedNASA = sorted(topNASA,key = lambda x:x[1],reverse=True)\n",
    "for item in sortedNASA:\n",
    "    print(item[0] + ': ' +str(item[1]))\n",
    "print('*'*10 + 'History' + '*'*10)\n",
    "sortedHis = sorted(topHis,key = lambda x:x[1],reverse=True)\n",
    "for item in sortedHis:\n",
    "    print(item[0] + ': ' +str(item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3337f217-fb56-497b-bed8-b9b3d6945311",
   "metadata": {},
   "source": [
    "# 总结\n",
    "- 对于分类而言使用概率比使用硬规则更加有效，贝叶斯准则提供一直利用已知数值估计未知概率的有效方法\n",
    "- 通过特征之间的条件独立性假设，降低对数据需求，当然这个假设过于简单因此被称为朴素贝叶斯。尽管该假设不正确，但其仍旧是一个有效的分类器\n",
    "- 利用编程语言实现NB需要解决很多实际问题，比如下溢，其可以通过取对数来改进。词袋模型在解决文档分类比词集模型更有效，还可以通过移除停用词和对切分器优化改进性能\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347c1ff-ac58-468f-966a-27733a16dc49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML] *",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
